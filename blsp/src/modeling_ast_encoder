from typing import Optional, Tuple
from dataclasses import dataclass
import os

import torch
from transformers import ASTConfig
from transformers.models.audio_spectrogram_transformer.modeling_audio_spectrogram_transformer import ASTModel as HFASTModel
from transformers.utils import ModelOutput

@dataclass
class ASTOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    output_lengths: Optional[torch.LongTensor] = None

def _get_feat_extract_output_lengths(input_lengths):
    return input_lengths




class ASTModel(HFASTModel):
    """
    overwrite forward to support attention_mask
    overwrite from_pretrained to support split encoder parameters from pretrained ASTModel
    """

    def from_pretrained(model_path):
        config = ASTConfig.from_pretrained(model_path)

        model = ASTModel(config)
        old_state_dict = torch.load(os.path.join(model_path, "pytorch_model.bin"))
        state_dict = {}
        for para_name in old_state_dict.keys():
            if "audio_spectrogram_transformer." in para_name:
                new_name = para_name.replace("audio_spectrogram_transformer.", "")
                state_dict[new_name] = old_state_dict[para_name]
        model.load_state_dict(state_dict)

        return model

    def forward(
        self,
        input_features,
        attention_mask=None,
        head_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        output = super().forward(
            input_features,
            head_mask,
            output_attentions,
            output_hidden_states,
            return_dict
        )

        if attention_mask is not None:
            last_hidden_state = output.last_hidden_state # B x T x C
            input_lengths = attention_mask.sum(-1)
            output_lengths = _get_feat_extract_output_lengths(input_lengths)
            max_length = output_lengths.max()
            last_hidden_state = last_hidden_state[:,:max_length,:]
        else:
            last_hidden_state = output.last_hidden_state # B x T x C
            bsz, time_step, _ = last_hidden_state.shape
            output_lengths = torch.LongTensor(bsz).fill_(1).to(last_hidden_state.device) * time_step

        return ASTOutput(
            last_hidden_state=last_hidden_state,
            hidden_states=None,
            attentions=None,
            output_lengths=output_lengths
        )
